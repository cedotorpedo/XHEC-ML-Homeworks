{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac188046",
   "metadata": {},
   "source": [
    "# Perceptron training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7add12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e30007",
   "metadata": {},
   "source": [
    "We consider the binary classification of labels $\\mathcal{Y}=\\{-1,1\\}$. A perceptron is a model based on a linear decision boundary given by $f_{w,b}(x)\\triangleq w^Tx+b$ and corresponding classifier $\\hat y_{w,b}(x)=\\text{sign}(f_{w,b}(x))$, which attempts to minimize the Bayes risk, defined for $f:\\mathbb{R}^d\\to \\{-1,1\\}$ by:\n",
    "\n",
    "$$\\mathcal{R}(f)\\triangleq \\mathbb{P}(f(X)\\neq Y)$$\n",
    "\n",
    "In the following we consider data sampled from a distribution $(X,Y)\\in \\mathbb{R}^d\\times\\{-1,1\\}$ and our goal is to solve:\n",
    "$$\\inf_{(w,b)\\in \\mathbb{R}^{d+1}}\\mathcal{R}(\\hat y_{w,b})\\quad (1).$$\n",
    "\n",
    "We propose to minimize the empirical risk obtained from $n$ iid samples:\n",
    "\n",
    "$$\\mathcal{R}_n(\\hat y_{w,b})\\triangleq \\frac 1n\\sum_{i=1}^n1_{\\hat y_{w,b}(X_i)\\neq Y_i}$$\n",
    "\n",
    "which leads to the minimization:\n",
    "\n",
    "$$\\inf_{(w,b)\\in \\mathbb{R}^{d+1}}\\mathcal{R}_n(\\hat y_{w,b})\\quad (2)$$.\n",
    "\n",
    "__Q1 (Minimization of the empirical risk):__ Compute $\\mathbb{E}[\\mathcal{R}_n(\\hat y_{w,b})]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7472f366",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f07223e6",
   "metadata": {},
   "source": [
    "__Q2 (Dataset generation):__ (a) Propose two functions to generate some data obtained from the following densities (verify they are densities!): \n",
    "\n",
    "$p_1(x,y)=\\frac 121_{y=-1}\\frac{1}{(2\\pi)^{d/2}4\\sigma^d}e^{-\\frac{\\Vert x\\Vert ^2}{32\\sigma^2}}+\\frac 121_{y=1}\\frac{1}{\\sigma^d(2\\pi)^{d/2}}e^{-\\frac{\\Vert x\\Vert ^2}{2\\sigma^2}}$ and $p_2(x,y)=\\frac 121_{y=-1}\\frac{1}{(2\\pi)^{d/2}\\sigma^d}e^{-\\frac{\\Vert x\\Vert^2}{2\\sigma^2}}+\\frac 121_{y=1}(\\frac 12\\frac{1}{(2\\pi)^{d/2}\\sigma^d}e^{-\\frac{\\Vert x-e_1\\Vert^2}{2\\sigma^2}}+\\frac 12\\frac{1}{(2\\pi)^{d/2}\\sigma^d}e^{-\\frac{\\Vert x-10e_{1}\\Vert^2}{2\\sigma^2}})$. Here, $e_1$ is the first vector of the canonical basis.\n",
    "\n",
    "(b) Plot 100 points samples from $p_1,p_2$ respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5bedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_distribution_1(d=2,N=1,sigma=1):\n",
    "    \"\"\"\n",
    "    Return the N samples following distribution p_1\n",
    "    \n",
    "    Parameters:\n",
    "        - d (int): ambient dimension.\n",
    "        - N (int): number of samples.\n",
    "        - sigma (float): variance of the distribution.\n",
    "    Returns:\n",
    "        - X (np.array): samples of shape (N,d).\n",
    "        - Y (np.array): labels with values in {-1,1} of shape (d).\n",
    "    \"\"\"\n",
    "\n",
    "   # FILL HERE\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X1,Y1=sample_from_distribution_1(N=100,sigma=10)\n",
    "\n",
    "\n",
    "plt.scatter(X1[Y1==1,0],X1[Y1==1,1],label='Data',c='r')\n",
    "plt.scatter(X1[Y1==-1,0],X1[Y1==-1,1],label='Data',c='b')\n",
    "plt.title('Samples from $p_1$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ff0dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_distribution_2(d=2,N=1,sigma=1):\n",
    "    \"\"\"\n",
    "    Return the N samples following distribution p_2\n",
    "    \n",
    "    Parameters:\n",
    "        - d (int): ambien dimension.\n",
    "        - N (int): number of samples.\n",
    "        - sigma (float): variance of the distribution.\n",
    "    Returns:\n",
    "        - X (np.array): samples of shape (N,d).\n",
    "        - Y (np.array): labels with values in {-1,1} of shape (d).\n",
    "    \"\"\"\n",
    "\n",
    "    # FILL HERE\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdf77c0",
   "metadata": {},
   "source": [
    "__Q3 (Classic perceptron (Rosenblatt, 1957)):__ The classical algorithm to minimize (1) is given by:\n",
    "        \n",
    "````\n",
    "for t=1..T\n",
    "   sample i in {1,...,n}\n",
    "   if Y_i(w^T.X_i+b)<0 then\n",
    "      w <- w + Y_i X_i\n",
    "      b <- b + Y_i b\n",
    "````\n",
    "\n",
    "Implement this strategy with a tracker of the training error accross iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f794713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classic_perceptron(X,Y,T):\n",
    "    \"\"\"\n",
    "    Train a perceptron using the Rosenblatt's algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        - X (np.array): input samples.\n",
    "        - Y (np.array): labels\n",
    "        - T (int): number of iterations\n",
    "    Returns:\n",
    "        - w (np.array): vector of size (d).\n",
    "        - b (np.array): bias of size (1).\n",
    "        - error (list): list of intermediate errors\n",
    "    \"\"\"\n",
    "    error=[]\n",
    "    w=np.zeros([X.shape[1]])\n",
    "    b=np.zeros([1])\n",
    "\n",
    "    # FILL HERE\n",
    "    \n",
    "    return w, b, error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25f3e4",
   "metadata": {},
   "source": [
    "__Q4 (Minimization via the quadratic loss):__ While the 0-1 loss is not differentiable, a classical strategy is to use a differentiable loss to approximate it. Here, we introduce the quadratic loss:\n",
    "$$\\ell(\\hat y,y)\\triangleq \\frac 12\\Vert y-\\hat y\\Vert^2\\,.$$\n",
    "\n",
    "For training, we will use the Stochastic Gradient Descent (SGD), in order to minimize the corresponding empirical risk given by:\n",
    "\n",
    "$$\\tilde{\\mathcal{R}}_n( f_{w,b})\\triangleq \\frac 1n \\sum_{i=1}^n \\ell(\\hat f_{w,b}(X_i),Y_i)\\,.$$\n",
    "\n",
    "An estimate of the gradient is defined as:\n",
    "$$g(w,b)\\triangleq \\nabla_{(w,b)}\\ell(\\hat y_{w,b}(X_\\mathbf{i}),Y_\\mathbf{i})$$ where $\\mathbf{i}$ is random variable, which corresponds to a uniform sampling over $\\{1,...,n\\}$.\n",
    "\n",
    "Then, once an initial point \n",
    "$(w_0,b_0)\\in \\mathbb{R}^{d+1}$ is picked, the following iterative descent procedure is followed, for some $\\eta>0$:\n",
    "\n",
    "$$(w_{n+1},b_{n+1})=(w_{n},b_{n})-\\eta g(w_{n},b_{n})\\quad(3)$$\n",
    "\n",
    "(a) Compute $g(w,b)$ and $\\mathbb{E}[g(w,b)]$ and propose a classifier.\n",
    "\n",
    "(b) Implement the descent of *(3)*, with a tracker of the training loss accross iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbac191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SGD_perceptron(X,Y,T,eta=0.1):\n",
    "    \"\"\"\n",
    "    Train a perceptron using SGD.\n",
    "    \n",
    "    Parameters:\n",
    "        - X (np.array): input samples.\n",
    "        - Y (np.array): labels\n",
    "        - T (int): number of iterations\n",
    "        - eta (float): step size (also known as learning rate)\n",
    "    Returns:\n",
    "        - w (np.array): vector of size (d).\n",
    "        - b (np.array): bias of size (1).\n",
    "        - error (list): list of intermediate errors\n",
    "    \"\"\"\n",
    "    error=[]\n",
    "    w=np.zeros([X.shape[1]])\n",
    "    b=np.zeros([1])\n",
    "    \n",
    "    # FILL HERE\n",
    "    \n",
    "    return w, b, error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73e938",
   "metadata": {},
   "source": [
    "__Q5:__ Test your model on both datasets for $d=2$. Plot the hyper-plane separator and the evolution of the training loss. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b28487",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X2\n",
    "Y=Y2\n",
    "\n",
    "# FILL HERE\n",
    "\n",
    "plt.scatter(X[Y==1,0],X[Y==1,1],label='Class 1',c='r')\n",
    "plt.scatter(X[Y==-1,0],X[Y==-1,1],label='Class 2',c='b')\n",
    "x=np.linspace(min(X[:,0]),max(X[:,0]))\n",
    "\n",
    "# FILL HERE\n",
    "\n",
    "plt.plot(x,y_c, label='Rosenblatt hyperplane')\n",
    "plt.plot(x,y_s, label='SGD hyperplane')\n",
    "plt.title('Samples from $p_2$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(error_c, label='Rosenblatt')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(error_s,label='SGD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4eaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X1\n",
    "Y=Y1\n",
    "\n",
    "# FILL HERE\n",
    "\n",
    "plt.scatter(X[Y==1,0],X[Y==1,1],label='Class 1',c='r')\n",
    "plt.scatter(X[Y==-1,0],X[Y==-1,1],label='Class 2',c='b')\n",
    "x=np.linspace(min(X[:,0]),max(X[:,0]))\n",
    "\n",
    "# FILL HERE\n",
    "\n",
    "plt.plot(x,y_c, label='Rosenblatt hyperplane')\n",
    "plt.plot(x,y_s, label='SGD hyperplane')\n",
    "plt.title('Samples from $p_1$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(error_c, label='Rosenblatt')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(error_s,label='SGD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71ab469",
   "metadata": {},
   "source": [
    "__Q6:__ Would you classify tanks on satellite images with a Perceptron? (source: https://www.jefftk.com/p/detecting-tanks )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1dff73",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
