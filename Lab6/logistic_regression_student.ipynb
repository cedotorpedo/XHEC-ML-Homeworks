{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a28c32b",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d4e3f",
   "metadata": {},
   "source": [
    "## Part 1: Training a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f3a386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from numpy.random import randint\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57bf0c",
   "metadata": {},
   "source": [
    "The sigmoïd function is defined by:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x)=\\frac{1}{1+\\exp(-x)}.\n",
    "\\end{align}\n",
    "    \n",
    "**Question 1:** Display the sigmoïd function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca572d67",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a6643",
   "metadata": {},
   "source": [
    "We consider the following functions for dataset generation of the previous session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398bcf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(sigma1=1., sigma2=1., theta=0.):\n",
    "    \"\"\"\n",
    "        Covariance matrix with eigenvalues sigma1 and sigma2, rotated by the angle theta.\n",
    "    \"\"\"\n",
    "    rotation = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                        [np.sin(theta), np.cos(theta)]])\n",
    "    cov = np.array([[sigma1, 0.],\n",
    "                   [0, sigma2]])\n",
    "    return rotation.dot(cov.dot(rotation.T))\n",
    "\n",
    "def gaussian_sample(mu=[0, 0], sigma1=1., sigma2=1., theta=0., n=50):\n",
    "    cov = covariance(sigma1, sigma2, theta)\n",
    "    x = multivariate_normal.rvs(mean=mu, cov=cov, size=n)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0356616",
   "metadata": {},
   "source": [
    "**Question 2:** Display a few samples and propose a solution to incorporate an affine bias term to a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb2e88",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec8c643",
   "metadata": {},
   "source": [
    "One of the most popular model in\n",
    "   $\\mathbb{R}^p$ is probably the **logistic model**, where\n",
    "   $$\n",
    "   \\mathbb{P}[Y=1|X=x]={\\exp\\left(\\langle \\beta^\\star,x\\rangle\\right)\\over 1+\\exp\\left(\\langle \\beta^\\star,x\\rangle\\right)}\\quad \\textrm{for all}\\ x\\in\\mathbb{R}^p,\n",
    "   $$\n",
    "   with $\\beta^\\star\\in\\mathbb{R}^p$. In this case, we have\n",
    "   $\\mathbb{P}[Y=1|X=x]>1/2$ if and only if $\\langle \\beta^\\star,x\\rangle>0$, so\n",
    "   the frontier between $\\left\\{x \\in \\mathbb R^p : \\mathscr{C}^\\star(x)=1\\right\\}$ and\n",
    "   $\\left\\{x \\in \\mathbb R^p : \\mathscr{C}^\\star(x)=0\\right\\}$ is a linear boundary, normal to $\\beta^*$.\n",
    "\n",
    "We can estimate the parameter $\\beta^\\star$ by maximizing the conditional likelihood of $(Y_1, \\dots, Y_n)$ given that $(X_1, \\dots, X_n) = (x_1, \\dots, x_n)$.\n",
    "\n",
    "**Question 3a:** Give the conditional likelihood that the logistic regression must maximize as well as the Bayes classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f2cbf",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a46bdd",
   "metadata": {},
   "source": [
    "In fact, we will rather consider the neg-log-likelihood $\\ell_{n}(\\beta)$ which is much easier to study. We might omit to write the dependency in $X,Y$.\n",
    "    \n",
    "**Question 3b:** We remind that $\\ell_n$ is obtained via:\n",
    "   \t$$\n",
    "   \t\\ell_{n}(\\beta)= -\\sum_{i=1}^n\\left[Y_{i}\\langle x_{i},\\beta\\rangle-\\log(1+\\exp(\\langle x_{i},\\beta\\rangle))\\right]\n",
    "    = -\\sum_{i=1}^n\\left[Y_{i}\\langle x_{i},\\beta\\rangle+\\log( \\sigma(-\\langle x_{i},\\beta\\rangle) )\\right].\n",
    "   \t$$\n",
    "Compute  the gradient $\\nabla\\ell_{n}(\\beta)$ and the Hessian $H_{n}(\\beta)$ of $\\ell_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117acc1",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df47c3-e6d6-4a1d-acc6-cb826edbc79b",
   "metadata": {},
   "source": [
    "**Question 4:** What can be said about $\\ell_n$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e986cb3",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959809f6",
   "metadata": {},
   "source": [
    "**Question 5**: Implement three functions to compute $\\ell_{n}(\\beta)$, $\\nabla\\ell_{n}(\\beta)$ and the Hessian $H_{n}(\\beta)$ for $\\beta\\in \\mathbb{R}^d$ and a given training set $(X,Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a6ff3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_loglikelihood(beta, X, Y, rho=0):\n",
    "    z = X@beta\n",
    "    return -np.sum(z*Y + np.log(sigmoid(-z)) + 0.5*rho*np.linalg.norm(beta)**2)\n",
    "\n",
    "def gradient_loglikelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed4f3c6",
   "metadata": {},
   "source": [
    "**_Hint:_** Test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec481725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value at a random point (must be non-negative)\n",
      "52.04206401024913\n",
      "Gradient first coordinate\n",
      "8.236676656281361\n",
      "8.236672668270279\n",
      "Hessian first coordinates\n",
      "7.968420764115081\n",
      "7.968428229885138\n"
     ]
    }
   ],
   "source": [
    "epsilon=0.000001\n",
    "beta=np.random.normal(size=3)\n",
    "beta_p=np.copy(beta)\n",
    "beta_p[0]+=epsilon\n",
    "print('Value at a random point (must be non-negative)')\n",
    "print(value_loglikelihood(beta,X,Y))\n",
    "# test grad\n",
    "print('Gradient first coordinate')\n",
    "print((value_loglikelihood(beta_p,X,Y)-value_loglikelihood(beta,X,Y))/epsilon)\n",
    "print(gradient_loglikelihood(beta,X,Y)[0])\n",
    "# test hessian\n",
    "print('Hessian first coordinates')\n",
    "print((gradient_loglikelihood(beta_p,X,Y)[0]-gradient_loglikelihood(beta,X,Y)[0])/epsilon)\n",
    "print(hessian_loglikelihood(beta,X,Y)[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b244eaa2",
   "metadata": {},
   "source": [
    "**Question 6:** We propose to introduce a Gaussian prior on the data, so that $p(\\beta)=p_{\\mathcal{N} \\left(0,\\rho^{-1} \\mathbf{I} \\right)}(\\beta)$. How does this affect the negative log-likelihood? What can be said about the convexity of this function? Implement the new corresponding functions, to evaluate its value, its gradient and its Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bd2ea0",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49c8c4f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa7534",
   "metadata": {},
   "source": [
    "### Method 1: First order method for minimizing $\\ell_n$ via Gradient Descent.\n",
    "\n",
    "A first order algorithm to train a logistic regression model consists in starting from a random point $\\beta_0\\in \\mathbb{R}^p$, fixing a step size $\\eta>0$ and number of iterations $T>0$ and iterate for $t<T$ as:\n",
    "$$\\beta_{t+1}=\\beta_t-\\eta \\nabla \\ell_n(\\beta_t).$$\n",
    "\n",
    "**Question 7:** Train your model and plot the corresponding training loss vs the iteration number and the linear separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd680a5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266e429",
   "metadata": {},
   "source": [
    "### Method 2: Newton Descent.\n",
    "\n",
    "A Newton gradient descent algorithm to train a logistic regression model consists in starting from a random point $\\beta_0\\in \\mathbb{R}^p$, fixing a step size $\\eta>0$ and number of iterations $T>0$ and iterate for $t<T$ as:\n",
    "$$\\beta_{t+1}=\\beta_t-\\eta H_{n}(\\beta_t)^{-1}\\nabla \\ell_n(\\beta_t).$$\n",
    "\n",
    "\n",
    "**Question 8**: Train your model, plot the corresponding training loss vs the iteration number and the linear separation. Compare to the previous solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a20fb3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12fb142-2b55-4ec4-90b9-3a4e05592a6d",
   "metadata": {},
   "source": [
    "### Method 3: Stochastic Gradient Descent\n",
    "\n",
    "A Stochastic Gradient Descent algorithm to train a logistic regression model consists in starting from a random point $\\beta_0\\in \\mathbb{R}^p$, fixing a step size $\\eta>0$ and number of iterations $T>0$ and iterate for $t<T$ as:\n",
    "$$\\beta_{t+1}=\\beta_t-\\nabla \\ell^\\rho(\\beta_t,X_{i_t},Y_{i_t}),$$\n",
    "where $i_t$ is independently sampled uniformly in $\\{1,...,n\\}$ at each iteration and\n",
    "$$\\ell^\\rho(\\beta,X,Y)\\triangleq \\left[Y\\langle X,\\beta\\rangle-\\log(1+\\exp(\\langle X,\\beta\\rangle))\\right]+\\frac{\\rho}2\\Vert\\beta\\Vert^2.$$\n",
    "\n",
    "\n",
    "**Question 9**: Train your model, plot the corresponding training loss vs the iteration number and the linear separation. Compare to the previous solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ebf57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d622d4e-6db4-4863-8885-4652124064eb",
   "metadata": {},
   "source": [
    "**Question 10**: Describe the complexity and rate of convergence of the three methods. How does $\\rho$ affect the convergence? Weight the pros and cons.\n",
    "\n",
    "**Hint:** When estimating $x^\\star \\in \\arg\\min f(x)$ and that $x^\\star$ is unknown, it can be useful to study $r_k=\\Vert\\nabla f(x_k)\\Vert^2$. Indeed, under the Polyak–Lojasiewicz (true for strongly convex functions), $\\Vert \\nabla f(x)\\Vert^2\\geq 2\\mu (f(x)-f(x^*))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff5a92f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740fe26b",
   "metadata": {},
   "source": [
    "## Part 2: Laplace approximation for Bayesian logistic regression\n",
    "\n",
    "We remind that we introduced a prior  $p(\\beta)$ on $\\beta$, which implies that $p(\\beta|\\mathcal{D})\\propto \\exp(-\\ell_n(\\beta))p(\\beta)=\\exp(-\\ell_n(\\beta)- \\frac{\\rho}{2}\\Vert \\beta\\Vert^2)=\\exp(-\\ell^\\rho_n(\\beta))$, and our goal is now to study the predictive posterior:\n",
    "\n",
    "\\begin{align}\n",
    "p(y_{\\text{new}}=1|x_{\\text{new}})=\\int_\\beta \\frac{\\exp(\\langle \\beta,x\\rangle)}{1+\\exp(\\langle \\beta,x\\rangle)}p(\\beta|\\mathcal{D})\\,d\\beta\n",
    "=\\int_\\beta \\sigma( \\langle \\beta,x\\rangle ) p(\\beta|\\mathcal{D})\\,d\\beta.\n",
    "\\end{align}\n",
    "\n",
    "However, sampling via $p(\\beta|\\mathcal{D})$ is a bit difficult here. Thus, our goal is to approximate $p(\\beta|\\mathcal{D})\\approx p_{\\mathcal{N}(\\mu,\\Sigma)}(\\beta)$. A Taylor expansion on $\\ell^\\rho_n$ gives:\n",
    "\n",
    "\\begin{align}\n",
    "\\ell^\\rho_n(\\beta)=\\ell_{n}^\\rho(\\beta_0)+(\\beta-\\beta_0)^T\\nabla \\ell_{n}(\\beta_0)+\\frac 12(\\beta-\\beta_0)^TH^\\rho_n(\\beta_0)(\\beta-\\beta_0)+o(\\Vert \\beta-\\beta_0\\Vert^2).\n",
    "\\end{align}\n",
    "\n",
    "**Question 11:** Find a (simple) condition on $\\beta_0$ such that $p(\\beta|\\mathcal{D})= p_{\\mathcal{N}(\\mu,\\Sigma)}(\\beta_0)+o(\\Vert \\beta-\\beta_0\\Vert^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dad0fa",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46f5af",
   "metadata": {},
   "source": [
    "**Question 12:** Compute an approximation of the predictive posterior $p(y_{new}=1 | x_{new})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f17735",
   "metadata": {},
   "source": [
    "We will use a Monte-Carlo strategy to approximate this quantity:\n",
    "$$p(y_{new}=1|x_{new}) \\approx \\mathbb{E}[\\sigma(W^Tx_{new})],$$ where $W\\sim \\mathcal{N}(\\hat \\beta,H_n^\\rho(\\hat\\beta)^{-1})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68512590",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
